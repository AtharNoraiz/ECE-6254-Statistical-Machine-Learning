{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import model_selection as sms\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(2018)\n",
    "n = 100\n",
    "xtrain = np.random.rand(n)\n",
    "ytrain = 0.25 + 0.5*xtrain + np.sqrt(0.1)*np.random.randn(n)\n",
    "idx = np.random.randint(0,100,10)\n",
    "ytrain[idx] = ytrain[idx] + np.random.randn(10)\n",
    "\n",
    "X_train, X_val, y_train, y_val = sms.train_test_split(xtrain, ytrain, test_size=0.33, random_state=76)\n",
    "\n",
    "X_train = X_train.reshape(-1,1) # as specified in the question\n",
    "X_val = X_val.reshape(-1,1)\n",
    "xtrain = xtrain.reshape(-1,1)\n",
    "\n",
    "yb = np.mean(ytrain) # to calculate the intercept from the slope\n",
    "xb = np.mean(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation scores are as follows: \n",
      "MSE for linear regression using lambda =  0.0001 is:  0.127828226975\n",
      "MSE for linear regression using lambda =  0.001 is:  0.127951690132\n",
      "MSE for linear regression using lambda =  0.01 is:  0.128343418317\n",
      "MSE for linear regression using lambda =  0.1 is:  0.129594639714\n",
      "MSE for linear regression using lambda =  1.0 is:  0.133659408019\n",
      "MSE for linear regression using lambda =  10.0 is:  0.147174012969\n",
      "MSE for linear regression using lambda =  100.0 is:  0.189541493075\n",
      "MSE for linear regression using lambda =  1000.0 is:  0.282489161873\n",
      "MSE for linear regression using lambda =  10000.0 is:  0.387152580671\n",
      "MSE for linear regression using lambda =  1000000.0 is:  0.474180874069\n",
      "The best lambda:  0.001\n",
      "Using Ridge regression, The slope is:  0.853749441218\n",
      "The intercept is:  0.0647156277696\n"
     ]
    }
   ],
   "source": [
    "# Ridge regression (lambda hyper parameter)\n",
    "print(\"Validation scores are as follows: \")\n",
    "lamset = np.array([0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 1000000])\n",
    "for k in range(0,10):\n",
    "    theta = np.dot(np.dot(np.linalg.inv(np.dot(np.transpose(X_train),X_train) + np.sqrt(lamset[k])),np.transpose(X_train)),y_train)\n",
    "    y_pred = np.dot(X_val, theta)\n",
    "    MSE = ((np.linalg.norm(y_val - y_pred))**2)/len(y_pred)\n",
    "    print(\"MSE for linear regression using lambda = \", lamset[k], \"is: \",  MSE)\n",
    "print(\"The best lambda: \", 0.001)\n",
    "theta = np.dot(np.dot(np.linalg.inv(np.dot(np.transpose(xtrain),xtrain) + 0.001),np.transpose(xtrain)),ytrain)\n",
    "print(\"Using Ridge regression, The slope is: \", theta[0])\n",
    "print(\"The intercept is: \" ,yb - (xb*theta[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation scores are as follows: \n",
      "The score:  0.221910197789 for epsilon =  1.1 alpha =  1e-07\n",
      "The score:  0.221910187828 for epsilon =  1.1 alpha =  1e-06\n",
      "The score:  0.221910088223 for epsilon =  1.1 alpha =  1e-05\n",
      "The score:  0.221909092174 for epsilon =  1.1 alpha =  0.0001\n",
      "The score:  0.221899132114 for epsilon =  1.1 alpha =  0.001\n",
      "The score:  0.221799577684 for epsilon =  1.1 alpha =  0.01\n",
      "The score:  0.220808227008 for epsilon =  1.1 alpha =  0.1\n",
      "The score:  0.209730233272 for epsilon =  1.1 alpha =  1.0\n",
      "The score:  0.13707129075 for epsilon =  1.1 alpha =  10.0\n",
      "The score:  0.0044014790649 for epsilon =  1.1 alpha =  100.0\n",
      "The score:  0.230401577375 for epsilon =  1.2 alpha =  1e-07\n",
      "The score:  0.230401567694 for epsilon =  1.2 alpha =  1e-06\n",
      "The score:  0.230401470882 for epsilon =  1.2 alpha =  1e-05\n",
      "The score:  0.230400502763 for epsilon =  1.2 alpha =  0.0001\n",
      "The score:  0.230390822053 for epsilon =  1.2 alpha =  0.001\n",
      "The score:  0.230294062998 for epsilon =  1.2 alpha =  0.01\n",
      "The score:  0.229331249688 for epsilon =  1.2 alpha =  0.1\n",
      "The score:  0.220132529517 for epsilon =  1.2 alpha =  1.0\n",
      "The score:  0.152805634627 for epsilon =  1.2 alpha =  10.0\n",
      "The score:  0.00139243050263 for epsilon =  1.2 alpha =  100.0\n",
      "The score:  0.231956512289 for epsilon =  1.25 alpha =  1e-07\n",
      "The score:  0.231956503801 for epsilon =  1.25 alpha =  1e-06\n",
      "The score:  0.231956418917 for epsilon =  1.25 alpha =  1e-05\n",
      "The score:  0.231955570079 for epsilon =  1.25 alpha =  0.0001\n",
      "The score:  0.231947081963 for epsilon =  1.25 alpha =  0.001\n",
      "The score:  0.231862226523 for epsilon =  1.25 alpha =  0.01\n",
      "The score:  0.231016240072 for epsilon =  1.25 alpha =  0.1\n",
      "The score:  0.222798386834 for epsilon =  1.25 alpha =  1.0\n",
      "The score:  0.151715907644 for epsilon =  1.25 alpha =  10.0\n",
      "The score:  -0.00252940923652 for epsilon =  1.25 alpha =  100.0\n",
      "The score:  0.192016453577 for epsilon =  1.5 alpha =  1e-07\n",
      "The score:  0.192016442756 for epsilon =  1.5 alpha =  1e-06\n",
      "The score:  0.192016334547 for epsilon =  1.5 alpha =  1e-05\n",
      "The score:  0.192015252462 for epsilon =  1.5 alpha =  0.0001\n",
      "The score:  0.192004431856 for epsilon =  1.5 alpha =  0.001\n",
      "The score:  0.191896250564 for epsilon =  1.5 alpha =  0.01\n",
      "The score:  0.190816787731 for epsilon =  1.5 alpha =  0.1\n",
      "The score:  0.180288978969 for epsilon =  1.5 alpha =  1.0\n",
      "The score:  0.110242209694 for epsilon =  1.5 alpha =  10.0\n",
      "The score:  -0.0200974582314 for epsilon =  1.5 alpha =  100.0\n",
      "The score:  0.173118234121 for epsilon =  1.75 alpha =  1e-07\n",
      "The score:  0.17311822411 for epsilon =  1.75 alpha =  1e-06\n",
      "The score:  0.173118123993 for epsilon =  1.75 alpha =  1e-05\n",
      "The score:  0.173117122826 for epsilon =  1.75 alpha =  0.0001\n",
      "The score:  0.173107111529 for epsilon =  1.75 alpha =  0.001\n",
      "The score:  0.173007035517 for epsilon =  1.75 alpha =  0.01\n",
      "The score:  0.172009959973 for epsilon =  1.75 alpha =  0.1\n",
      "The score:  0.162397090115 for epsilon =  1.75 alpha =  1.0\n",
      "The score:  0.0934310841731 for epsilon =  1.75 alpha =  10.0\n",
      "The score:  -0.0332266664185 for epsilon =  1.75 alpha =  100.0\n",
      "The score:  0.229877092752 for epsilon =  1.3 alpha =  1e-07\n",
      "The score:  0.229877084434 for epsilon =  1.3 alpha =  1e-06\n",
      "The score:  0.229877001252 for epsilon =  1.3 alpha =  1e-05\n",
      "The score:  0.229876169436 for epsilon =  1.3 alpha =  0.0001\n",
      "The score:  0.229867851393 for epsilon =  1.3 alpha =  0.001\n",
      "The score:  0.229784682126 for epsilon =  1.3 alpha =  0.01\n",
      "The score:  0.228954094184 for epsilon =  1.3 alpha =  0.1\n",
      "The score:  0.220678385305 for epsilon =  1.3 alpha =  1.0\n",
      "The score:  0.148562364854 for epsilon =  1.3 alpha =  10.0\n",
      "The score:  -0.00891504683266 for epsilon =  1.3 alpha =  100.0\n",
      "The score:  0.213752811382 for epsilon =  1.4 alpha =  1e-07\n",
      "The score:  0.213752801631 for epsilon =  1.4 alpha =  1e-06\n",
      "The score:  0.213752704125 for epsilon =  1.4 alpha =  1e-05\n",
      "The score:  0.213751729068 for epsilon =  1.4 alpha =  0.0001\n",
      "The score:  0.213741978608 for epsilon =  1.4 alpha =  0.001\n",
      "The score:  0.213644485533 for epsilon =  1.4 alpha =  0.01\n",
      "The score:  0.212670712839 for epsilon =  1.4 alpha =  0.1\n",
      "The score:  0.203180618627 for epsilon =  1.4 alpha =  1.0\n",
      "The score:  0.127096047521 for epsilon =  1.4 alpha =  10.0\n",
      "The score:  -0.0160385089934 for epsilon =  1.4 alpha =  100.0\n",
      "The score:  0.192016453577 for epsilon =  1.5 alpha =  1e-07\n",
      "The score:  0.192016442756 for epsilon =  1.5 alpha =  1e-06\n",
      "The score:  0.192016334547 for epsilon =  1.5 alpha =  1e-05\n",
      "The score:  0.192015252462 for epsilon =  1.5 alpha =  0.0001\n",
      "The score:  0.192004431856 for epsilon =  1.5 alpha =  0.001\n",
      "The score:  0.191896250564 for epsilon =  1.5 alpha =  0.01\n",
      "The score:  0.190816787731 for epsilon =  1.5 alpha =  0.1\n",
      "The score:  0.180288978969 for epsilon =  1.5 alpha =  1.0\n",
      "The score:  0.110242209694 for epsilon =  1.5 alpha =  10.0\n",
      "The score:  -0.0200974582314 for epsilon =  1.5 alpha =  100.0\n"
     ]
    }
   ],
   "source": [
    "# Using Huber loss (epsilon and alpha hyperparameters)\n",
    "from sklearn import linear_model\n",
    "X_train, X_val, y_train, y_val = sms.train_test_split(xtrain, ytrain, test_size=0.33, random_state=76)\n",
    "eps = np.array([1.1, 1.2, 1.25, 1.50, 1.75, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8])\n",
    "alp = np.array([0.0000001, 0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100])\n",
    "print(\"Validation scores are as follows: \")\n",
    "for k in range(0,8):\n",
    "    for j in range(0,10): \n",
    "        reg = linear_model.HuberRegressor(epsilon = eps[k], alpha=alp[j])\n",
    "        reg.fit(X_train,y_train)\n",
    "        scoree = reg.score(X_val, y_val)\n",
    "        print(\"The score: \", reg.score(X_val, y_val), \"for epsilon = \", eps[k], \"alpha = \", alp[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Huber loss, Slope:  0.523816794135 and Intercept:  0.24089946044 for the best model\n"
     ]
    }
   ],
   "source": [
    "# Best alpha = 10-7 and epsilon = 1.25 \n",
    "reg2 = linear_model.HuberRegressor(epsilon = 1.25, alpha=0.0000001)\n",
    "reg2.fit(xtrain,ytrain)\n",
    "print(\"Using Huber loss, Slope: \", reg2.coef_[0], \"and Intercept: \", reg2.intercept_, \"for the best model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation scores are as follows: \n",
      "The score:  -0.0415796481774 for epsilon =  0.001 C =  1e-06\n",
      "The score:  -0.0399196232595 for epsilon =  0.001 C =  1e-05\n",
      "The score:  -0.0550491997889 for epsilon =  0.001 C =  0.0001\n",
      "The score:  -0.0625412517522 for epsilon =  0.001 C =  0.001\n",
      "The score:  -0.106242784192 for epsilon =  0.001 C =  0.01\n",
      "The score:  -0.0628162683496 for epsilon =  0.001 C =  0.1\n",
      "The score:  -0.129822508637 for epsilon =  0.001 C =  1.0\n",
      "The score:  -0.0223067212647 for epsilon =  0.001 C =  10.0\n",
      "The score:  -0.0415384229249 for epsilon =  0.01 C =  1e-06\n",
      "The score:  -0.0398905041085 for epsilon =  0.01 C =  1e-05\n",
      "The score:  -0.0549946464896 for epsilon =  0.01 C =  0.0001\n",
      "The score:  -0.0625061197774 for epsilon =  0.01 C =  0.001\n",
      "The score:  -0.106202057799 for epsilon =  0.01 C =  0.01\n",
      "The score:  -0.0627955221743 for epsilon =  0.01 C =  0.1\n",
      "The score:  -0.129821770798 for epsilon =  0.01 C =  1.0\n",
      "The score:  -0.0223082536815 for epsilon =  0.01 C =  10.0\n",
      "The score:  -0.0411263104132 for epsilon =  0.1 C =  1e-06\n",
      "The score:  -0.0395994408192 for epsilon =  0.1 C =  1e-05\n",
      "The score:  -0.0544493388437 for epsilon =  0.1 C =  0.0001\n",
      "The score:  -0.062154902942 for epsilon =  0.1 C =  0.001\n",
      "The score:  -0.105794924784 for epsilon =  0.1 C =  0.01\n",
      "The score:  -0.0625880972014 for epsilon =  0.1 C =  0.1\n",
      "The score:  -0.129814405203 for epsilon =  0.1 C =  1.0\n",
      "The score:  -0.0223235780394 for epsilon =  0.1 C =  10.0\n",
      "The score:  -0.0370191866092 for epsilon =  0.2 C =  1e-06\n",
      "The score:  -0.0367016300597 for epsilon =  0.2 C =  1e-05\n",
      "The score:  -0.0490187970798 for epsilon =  0.2 C =  0.0001\n",
      "The score:  -0.0586530258143 for epsilon =  0.2 C =  0.001\n",
      "The score:  -0.101736686828 for epsilon =  0.2 C =  0.01\n",
      "The score:  -0.0605175254419 for epsilon =  0.2 C =  0.1\n",
      "The score:  -0.129742028908 for epsilon =  0.2 C =  1.0\n",
      "The score:  -0.0224768406217 for epsilon =  0.2 C =  10.0\n",
      "The score:  -0.00638733586035 for epsilon =  0.3 C =  1e-06\n",
      "The score:  -0.00861325152578 for epsilon =  0.3 C =  1e-05\n",
      "The score:  0.00349876684113 for epsilon =  0.3 C =  0.0001\n",
      "The score:  -0.0258289435323 for epsilon =  0.3 C =  0.001\n",
      "The score:  -0.0624635259166 for epsilon =  0.3 C =  0.01\n",
      "The score:  -0.0418450324285 for epsilon =  0.3 C =  0.1\n",
      "The score:  -0.12914623085 for epsilon =  0.3 C =  1.0\n",
      "The score:  -0.0240113667675 for epsilon =  0.3 C =  10.0\n",
      "The score:  0.108358692749 for epsilon =  0.4 C =  1e-06\n",
      "The score:  0.117194285425 for epsilon =  0.4 C =  1e-05\n",
      "The score:  0.143880605013 for epsilon =  0.4 C =  0.0001\n",
      "The score:  0.130450535954 for epsilon =  0.4 C =  0.001\n",
      "The score:  0.0208327191592 for epsilon =  0.4 C =  0.01\n",
      "The score:  0.0189300105092 for epsilon =  0.4 C =  0.1\n",
      "The score:  -0.11495274995 for epsilon =  0.4 C =  1.0\n",
      "The score:  -0.0395466605199 for epsilon =  0.4 C =  10.0\n",
      "The score:  0.203026955814 for epsilon =  0.5 C =  1e-06\n",
      "The score:  0.210826797629 for epsilon =  0.5 C =  1e-05\n",
      "The score:  0.205931593941 for epsilon =  0.5 C =  0.0001\n",
      "The score:  0.236880460588 for epsilon =  0.5 C =  0.001\n",
      "The score:  0.142009454424 for epsilon =  0.5 C =  0.01\n",
      "The score:  0.0355837336682 for epsilon =  0.5 C =  0.1\n",
      "The score:  -0.0731274435304 for epsilon =  0.5 C =  1.0\n",
      "The score:  -0.213902827475 for epsilon =  0.5 C =  10.0\n",
      "The score:  0.221691749298 for epsilon =  1.0 C =  1e-06\n",
      "The score:  0.211605598348 for epsilon =  1.0 C =  1e-05\n",
      "The score:  0.228928362644 for epsilon =  1.0 C =  0.0001\n",
      "The score:  0.246377894762 for epsilon =  1.0 C =  0.001\n",
      "The score:  0.142008883172 for epsilon =  1.0 C =  0.01\n",
      "The score:  0.0355846397761 for epsilon =  1.0 C =  0.1\n",
      "The score:  -0.0731266049205 for epsilon =  1.0 C =  1.0\n",
      "The score:  -0.338958823038 for epsilon =  1.0 C =  10.0\n"
     ]
    }
   ],
   "source": [
    "# Using SVR\n",
    "from sklearn.svm import SVR\n",
    "X_train, X_val, y_train, y_val = sms.train_test_split(xtrain, ytrain, test_size=0.33, random_state=76)\n",
    "eps = np.array([0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 1.0, 10, 100])\n",
    "C = np.array([0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000])\n",
    "print(\"Validation scores are as follows: \")\n",
    "for k in range(0,8):\n",
    "    for j in range(0,8): \n",
    "        reg = SVR(C=C[k], epsilon=eps[j],kernel = 'linear')\n",
    "        reg.fit(X_train,y_train)\n",
    "        print(\"The score: \", reg.score(X_val, y_val), \"for epsilon = \", eps[k], \"C = \", C[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slope using SVR:  [-0.0002004]\n",
      "The intercept using SVR:  0.51277886889\n"
     ]
    }
   ],
   "source": [
    "# Best epsilon = 1 and C = 0.001\n",
    "reg3 = SVR(C=0.001, epsilon=1, kernel='linear')\n",
    "reg3.fit(xtrain,ytrain)\n",
    "print(\"The slope using SVR: \", reg3.coef_[0])\n",
    "print(\"The intercept using SVR: \", reg3.intercept_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f360b2ada20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(xtrain, ytrain, 'rx') # scatter plot of all the data points \n",
    "plt.plot(xtrain, xtrain*theta[0] + inter1, 'r') # ridge regression line (red)\n",
    "plt.plot(xtrain, xtrain*reg2.coef_[0] + reg2.intercept_, 'b') # Huber loss line (blue)\n",
    "plt.plot(xtrain, xtrain*reg3.coef_[0] + reg3.intercept_, 'k') # SVR line (black)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
