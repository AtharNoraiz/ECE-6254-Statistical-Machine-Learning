{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To compare the performance of least squares, ridge regression and LASSO.\n",
    "# Ideally, we should get accuracy increase in the same order.\n",
    "from sklearn.datasets import load_boston\n",
    "import numpy as np\n",
    "from sklearn import preprocessing as skp\n",
    "from sklearn import model_selection as sms\n",
    "boston = load_boston()\n",
    "X_orig = boston.data\n",
    "y = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n",
      "(506,)\n"
     ]
    }
   ],
   "source": [
    "# The scaling and shifting need to be done for both the training and testing set but the information on how to \n",
    "# scale and shift must only be gleaned from the training set.\n",
    "print(X_orig.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for standardizing by subtracting mean and unit variance...\n",
    "# Do we have to decide the standardization method? \n",
    "\n",
    "X = X_orig\n",
    "# X = skp.scale(X_orig)\n",
    "# print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 13)\n",
      "(106, 13)\n",
      "(400,)\n",
      "(106,)\n",
      "(300, 13)\n",
      "(100, 13)\n",
      "(300,)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = sms.train_test_split(X, y, test_size=0.208, random_state=76)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "mean_vec = np.mean(X_train, axis = 0)\n",
    "std_vec = np.std(X_train, axis = 0)\n",
    "\n",
    "mean_vecy = np.mean(y_train, axis = 0)\n",
    "std_vecy = np.std(y_train, axis = 0)\n",
    "\n",
    "# Need to define holdout sets for defining hyperparameters in the case of ridge regression and for LASSO. \n",
    "X_train_rr_lasso, X_val, y_train_rr_lasso, y_val = sms.train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "print(X_train_rr_lasso.shape)\n",
    "print(X_val.shape)\n",
    "print(y_train_rr_lasso.shape)\n",
    "print(y_val.shape)\n",
    "\n",
    "X_train = (X_train - mean_vec)/(std_vec)\n",
    "X_test = (X_test - mean_vec)/(std_vec)\n",
    "X_train_rr_lasso = (X_train_rr_lasso - mean_vec)/(std_vec)\n",
    "X_val = (X_val - mean_vec)/(std_vec)\n",
    "X_train = np.column_stack((X_train,np.ones(len(X_train))))\n",
    "X_test = np.column_stack((X_test,np.ones(len(X_test))))\n",
    "X_val = np.column_stack((X_val,np.ones(len(X_val))))\n",
    "X_train_rr_lasso = np.column_stack((X_train_rr_lasso,np.ones(len(X_train_rr_lasso))))\n",
    "\n",
    "\n",
    "y_train = (y_train - mean_vecy)#/(std_vecy)\n",
    "y_test = (y_test - mean_vecy)#/(std_vecy)\n",
    "y_train_rr_lasso = (y_train_rr_lasso - mean_vecy)#/(std_vecy)\n",
    "y_val = (y_val - mean_vecy)#/(std_vecy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for normal linear regression:  16.34860534\n"
     ]
    }
   ],
   "source": [
    "theta = np.dot(np.dot((np.linalg.inv(np.dot(np.transpose(X_train),X_train))),np.transpose(X_train)),y_train)\n",
    "y_pred = np.dot(X_test, theta)\n",
    "MSE = ((np.linalg.norm(y_test - y_pred))**2)/len(y_pred)\n",
    "print(\"MSE for normal linear regression: \", MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation scores are as follows: \n",
      "MSE for linear regression using lambda =  1e-06 is:  22.8850062702\n",
      "MSE for linear regression using lambda =  1e-05 is:  22.8850063597\n",
      "MSE for linear regression using lambda =  0.0001 is:  22.8850072547\n",
      "MSE for linear regression using lambda =  0.001 is:  22.8850162036\n",
      "MSE for linear regression using lambda =  0.01 is:  22.8851055687\n",
      "MSE for linear regression using lambda =  0.1 is:  22.8859868249\n",
      "MSE for linear regression using lambda =  1.0 is:  22.8935623472\n",
      "MSE for linear regression using lambda =  10.0 is:  22.8750069751\n",
      "MSE for linear regression using lambda =  100.0 is:  22.7779605975\n",
      "MSE for linear regression using lambda =  1000.0 is:  39.474646544\n",
      "MSE for linear regression using lambda =  10000.0 is:  72.6952759129\n",
      "MSE for linear regression using lambda =  1000000.0 is:  85.0336221078\n",
      "The best lambda:  100\n",
      "Ridge egression model trained on the entire set: \n",
      "MSE Score using best lambda =  100 :  15.5953601622\n"
     ]
    }
   ],
   "source": [
    "# Ridge regression (lambda hyper parameter)\n",
    "print(\"Validation scores are as follows: \")\n",
    "lamset = np.array([0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 1000000])\n",
    "for k in range(0,12):\n",
    "    theta = np.dot(np.dot((np.linalg.inv(np.dot(np.transpose(X_train_rr_lasso),X_train_rr_lasso) + lamset[k]*np.identity(14))),np.transpose(X_train_rr_lasso)),y_train_rr_lasso)\n",
    "    y_pred = np.dot(X_val, theta)\n",
    "    MSE = ((np.linalg.norm(y_val - y_pred))**2)/len(y_pred)\n",
    "    print(\"MSE for linear regression using lambda = \", lamset[k], \"is: \",  MSE)\n",
    "print(\"The best lambda: \", 100)\n",
    "print(\"Ridge egression model trained on the entire set: \")\n",
    "theta = np.dot(np.dot((np.linalg.inv(np.dot(np.transpose(X_train),X_train) + 100*np.identity(14))),np.transpose(X_train)),y_train)\n",
    "y_pred = np.dot(X_test, theta)\n",
    "MSE = ((np.linalg.norm(y_test - y_pred))**2)/len(y_pred)\n",
    "print(\"MSE Score using best lambda = \",100, ': ', MSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation scores are as follows: \n",
      "Score for alpha =  1e-07 :  0.731298726628\n",
      "Score for alpha =  1e-06 :  0.731298655974\n",
      "Score for alpha =  1e-05 :  0.731297949379\n",
      "Score for alpha =  0.0001 :  0.731290867182\n",
      "Score for alpha =  0.001 :  0.731219033436\n",
      "Score for alpha =  0.01 :  0.730397253034\n",
      "Score for alpha =  0.1 :  0.723165756075\n",
      "Score for alpha =  1.0 :  0.692256413639\n",
      "Score for alpha =  10.0 :  -0.000414986887945\n",
      "Score for alpha =  100.0 :  -0.000414986887945\n",
      "The best alpha:  0.01\n",
      "Regression model trained on the entire set: \n",
      "Score using best alpha =  0.01 :  0.661225050523\n",
      "The coefficients are:  [-0.9875882   1.17407324  0.30151731  0.77439062 -1.85282595  2.60925289\n",
      " -0.         -2.93360802  2.77892596 -2.07389602 -2.07915671  0.9640536\n",
      " -4.27219691  0.        ]\n",
      "No. of non-zero elements in theta:  12\n"
     ]
    }
   ],
   "source": [
    "# LASSO (alpha hyperparameter)\n",
    "print(\"Validation scores are as follows: \")\n",
    "from sklearn import linear_model\n",
    "alphaset = np.array([0.0000001, 0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100])\n",
    "for k in range(0,10):\n",
    "    reg = linear_model.Lasso(alpha = alphaset[k])\n",
    "    reg.fit(X_train_rr_lasso,y_train_rr_lasso)\n",
    "    print(\"Score for alpha = \",alphaset[k], ': ', reg.score(X_val, y_val))\n",
    "# need to compare this with y_val for different iterations of alpha. With the best alpha, need to train it again \n",
    "# on X_train and y_train and predict on X_test. \n",
    "print(\"The best alpha: \", 0.01)\n",
    "print(\"Regression model trained on the entire set: \")\n",
    "reg1 = linear_model.Lasso(alpha = 0.01)\n",
    "reg1.fit(X_train,y_train)\n",
    "print(\"Score using best alpha = \",0.01, ': ', reg1.score(X_test, y_test))\n",
    "print(\"The coefficients are: \", reg1.coef_)\n",
    "print(\"No. of non-zero elements in theta: \", len(reg1.coef_.nonzero()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for LASSO regression:  16.2357020162\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.dot(X_test, reg1.coef_)\n",
    "MSE = ((np.linalg.norm(y_test - y_pred))**2)/len(y_pred)\n",
    "print(\"MSE for LASSO regression: \", MSE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
